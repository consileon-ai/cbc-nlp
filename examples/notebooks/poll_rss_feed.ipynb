{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from consileon.nlp.rss_scraping import RssScraper\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create content from RSS feeds\n",
    "\n",
    "Rss provide a simple method to get text data with a rather high quality with little work.\n",
    "\n",
    "The class ``RssScraper`` pull the current news items from an rss feed, read the referenced content\n",
    "in each item, add the content to the feed item and stores it to disc for later usage.\n",
    "\n",
    "You have to implement the function with actually \"scrapes\" the content from the ``html``\n",
    "(or ``pdf`` or whatever)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_dir = \"../sample_data/rss_feeds\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory ../sample_data/rss_feeds/rki/ failed\n"
     ]
    }
   ],
   "source": [
    "rki_urls = [\"https://www.rki.de/SiteGlobals/Functions/RSSFeed/RSSGenerator_nCoV.xml\"]\n",
    "\n",
    "\n",
    "def rki_extractor(an_html) :\n",
    "    soup = bs(an_html, \"lxml\")\n",
    "    content = soup.find('div', attrs={\"id\": \"content\"})\n",
    "    text = \"\\n\\n\".join([ p.get_text() for p in content.find_all(['p', 'h', 'h1', 'h2', 'h3', 'title']) ])\n",
    "    return text.strip()\n",
    "\n",
    "rki = RssScraper(rki_urls, base_dir + \"/rki/\", rki_extractor, 1 * 3600, True)\n",
    "\n",
    "rki.pull_once()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory ../sample_data/rss_feeds/bw/ failed\n"
     ]
    }
   ],
   "source": [
    "bw_urls = [\"https://www.baden-wuerttemberg.de/de/service/rss/xml/rss-alle-meldungen/\"]\n",
    "\n",
    "def bw_extractor(an_html) :\n",
    "    soup = bs(an_html, \"lxml\")\n",
    "    content = soup.find('div', attrs={\"data-rtr-content\": \"#read\"})\n",
    "    text = \"\\n\\n\".join([ p.get_text() for p in content.find_all(['p', 'h', 'h1', 'h2', 'h3', 'title']) ])\n",
    "    return text.strip()\n",
    "\n",
    "bw = RssScraper(bw_urls, base_dir + \"/bw/\", bw_extractor, 1 * 3600, True)\n",
    "\n",
    "bw.pull_once()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}