Alternative Schreibweisen sind unter anderem die Ursprungsvariante ''Allen Smithee'' sowie ''Alan Smythee'' und ''Adam Smithee''. Auch zwei teilweise asiatisch anmutende Schreibweisen ''Alan Smi Thee'' und ''Sumishii Aran'' gehören – so die Internet Movie Database – dazu. Das Pseudonym entstand 1968 infolge der Arbeiten am Western-Film ''Death of a Gunfighter'' (deutscher Titel ''Frank Patch – Deine Stunden sind gezählt''). Regisseur Robert Totten und Hauptdarsteller Richard Widmark gerieten in einen Streit, woraufhin Don Siegel als neuer Regisseur eingesetzt wurde. Der Film trug nach Abschluss der Arbeiten noch deutlich Tottens Handschrift, der auch mehr Drehtage als Siegel daran gearbeitet hatte, weshalb dieser die Nennung seines Namens als Regisseur ablehnte. Totten selbst lehnte aber ebenfalls ab. Als Lösung wurde  ''Allen Smithee'' als ein möglichst einzigartiger Name gewählt (bei der späteren Variante ''Alan Smithee'' war das Anagramm ''The Alias Men'' vermutlich kein Entstehungsgrund). In den zeitgenössischen Kritiken wurde der Regisseur u. a. von Roger Ebert mit den Worten gelobt: 1997 kam die Parodie ''An Alan Smithee Film: Burn Hollywood Burn'' (deutscher Titel ''Fahr zur Hölle Hollywood'') in die Kinos, was das Pseudonym einem größeren Publikum bekannt machte, nicht zuletzt weil Arthur Hiller, der eigentliche Regisseur des Films, selbst seinen Namen zurückzog und analog zum Filmtitel das Pseudonym ''Alan Smithee'' benutzte. Der Film gilt als einer der schlechtesten Filme der 1990er Jahre und gewann fünf Goldene Himbeeren. Der Film ''Supernova'' ist der erste Post-Smithee-Film, dort führte ein gewisser ''Thomas Lee'' alias Walter Hill die Regie. Die Verwendung dieses oder eines anderen Pseudonyms ist für Mitglieder der DGA streng reglementiert. Ein Regisseur, der für einen von ihm gedrehten Film seinen Namen nicht hergeben möchte, hat nach Sichtung des fertigen Films drei Tage Zeit, anzuzeigen, dass er ein Pseudonym verwenden möchte. Der Rat der DGA entscheidet binnen zwei Tagen über das Anliegen. Erhebt die Produktionsfirma Einspruch, entscheidet ein Komitee aus Mitgliedern der DGA und der Vereinigung der Film- und Fernsehproduzenten, ob der Regisseur ein Pseudonym angeben darf. Über die Beantragung muss der Regisseur Stillschweigen halten, ebenso darf er den fertigen Film nicht öffentlich kritisieren, wenn die DGA ihm die Verwendung eines Pseudonyms zugesteht. Ein Antrag des Regisseurs auf Pseudonymisierung kann abgelehnt werden, so durfte Tony Kaye den Namen Smithee bei dem Film ''American History X'' nicht einsetzen, obwohl er den Antrag stellte. Auch bei nicht-US-amerikanischen Produktionen wird der Name verwendet, wie etwa beim Pilotfilm der Fernsehserie ''Schulmädchen''. 2007 sendete die ARD am 8. und 9. August den zweiteiligen TV-Film ''Paparazzo''. Auch in diesem Werk erscheint anstatt des eigentlichen Regisseurs Stephan Wagner Alan Smithee im Abspann. Regisseure, die das Pseudonym benutzt haben: Der Pilotfilm der Serie ''MacGyver'' und die fünfte Folge der ersten Staffel führen einen Alan Smithee als Regisseur. Auf der TV-Serien-Seite ''TV Rage'' wird Jerrold Freedman als Regisseur des Pilotfilms angegeben. Der Regisseur der fünften Folge ist unbekannt. Zu den Drehbuchautoren, die das Pseudonym benutzt haben, gehören Sam Raimi und Ivan Raimi, die das Drehbuch zu ''Die total beknackte Nuß'' als ''Alan Smithee, Jr.'' und ''Alan Smithee, Sr.'' schrieben. Auch in Computerspielen wird dieses Pseudonym angegeben: Im Abspann des Ego-Shooters ''Marine Sharpshooter IV'' aus dem Jahr 2008 wird als Art Director des Spiels ''Alan Smithee'' genannt. 2014 produzierte die New Yorker Performance-Kompanie Big Dance Theater ''Alan Smithee Directed this Play'', das im August des Jahres auch in Berlin bei Tanz im August aufgeführt wurde.
Das Actinium wurde im Jahr 1899 von dem französischen Chemiker André-Louis Debierne entdeckt, der es aus Pechblende isolierte und ihm zunächst Ähnlichkeiten mit dem Titan oder dem Thorium zuschrieb; seine Bezeichnung leitete er wegen der Radioaktivität von griechisch ἀκτίς ''aktís'' ‚Strahl‘ ab. Friedrich Giesel entdeckte das Element unabhängig davon im Jahr 1902 und beschrieb eine Ähnlichkeit zum Lanthan; er gab ihm den Namen '''Emanium''', eine Bildung zu lateinisch ''emano'' ‚ausfließen‘, ebenfalls mit Bezug zur abgegebenen Strahlung. Nachdem Actinium und Emanium im Jahre 1904 als identisch erkannt worden waren, wurde Debiernes Namensgebung der Vorzug gegeben, da er es zuerst entdeckt hatte. Die Geschichte der Entdeckung wurde in Publikationen von 1971 und später im Jahr 2000 immer noch als fraglich beschrieben. Sie zeigen, dass die Publikationen von 1904 einerseits und die von 1899 und 1900 andererseits Widersprüche aufweisen. Da in Uranerzen nur wenig Actinium vorhanden ist, spielt diese Quelle keine Rolle für die Gewinnung. Technisch wird das Isotop 227Ac durch Bestrahlung von 226Ra mit Neutronen in Kernreaktoren hergestellt. Durch den schnellen Zerfall des Actiniums waren stets nur geringe Mengen verfügbar. Die erste künstliche Herstellung von Actinium wurde im Argonne National Laboratory in Chicago durchgeführt. Das Metall ist silberweiß glänzend und relativ weich. Aufgrund seiner starken Radioaktivität leuchtet Actinium im Dunkeln in einem hellblauen Licht. Actinium ist das namensgebende Element der Actinoiden, ähnlich wie Lanthan für die Lanthanoiden. Die Gruppe der Elemente zeigt deutlichere Unterschiede als die Lanthanoide; daher dauerte es bis 1945, bis Glenn T. Seaborg die wichtigsten Änderungen zum Periodensystem von Mendelejew vorschlagen konnte: die Einführung der Actinoide. Es ist sehr reaktionsfähig und wird von Luft und Wasser angegriffen, überzieht sich aber mit einer Schicht von Actiniumoxid, wodurch es vor weiterer Oxidation geschützt ist. Das Ac3+-Ion ist farblos. Das chemische Verhalten von Actinium ähnelt sehr dem Lanthan. Actinium ist in allen zehn bekannten Verbindungen dreiwertig. Bekannt sind 26 Isotope, wovon nur zwei natürlich vorkommen. Das langlebigste Isotop 227Ac (Halbwertszeit 21,8 Jahre) hat zwei Zerfallskanäle: es ist ein Alpha- und Beta-Strahler. 227Ac ist ein Zerfallsprodukt des Uranisotops 235U und kommt zu einem kleinen Teil in Uranerzen vor. Daraus lassen sich wägbare Mengen 227Ac gewinnen, die somit ein verhältnismäßig einfaches Studium dieses Elementes ermöglichen. Da sich unter den radioaktiven Zerfallsprodukten einige Gammastrahler befinden, sind aber aufwändige Strahlenschutzvorkehrungen nötig. Actinium wird zur Erzeugung von Neutronen eingesetzt, die bei Aktivierungsanalysen eine Rolle spielen. Außerdem wird es für die thermoionische Energieumwandlung genutzt. Beim dualen Zerfall des 227Ac geht der größte Teil unter Emission von Beta-Teilchen in das Thoriumisotop 227Th, aber ca. 1 % zerfällt durch Alpha-Emission zu Francium 223Fr. Eine Lösung von 227Ac ist daher als Quelle für das kurzlebige 223Fr verwendbar. Letzteres kann dann regelmäßig abgetrennt und untersucht werden. Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt. Nur eine geringe Anzahl von Actiniumverbindungen ist bekannt. Mit Ausnahme von AcPO4 sind sie alle den entsprechenden Lanthanverbindungen ähnlich und enthalten Actinium in der Oxidationsstufe +3. Insbesondere unterscheiden sich die Gitterkonstanten der jeweiligen Lanthan- und Actinium-Verbindungen nur in wenigen Prozent. Actinium(III)-oxid (Ac2O3) kann durch Erhitzen des Hydroxids bei 500 °C oder des Oxalats bei 1100 °C im Vakuum erhalten werden. Das Kristallgitter ist isotyp mit den Oxiden der meisten dreiwertigen Seltenerdmetalle. Actinium(III)-fluorid (AcF3) kann entweder in Lösung oder durch Feststoffreaktion dargestellt werden. Im ersten Fall gibt man bei Raumtemperatur Flusssäure zu einer Ac3+-Lösung und fällt das Produkt aus. im anderen Fall wird Actinium-Metall mit Fluorwasserstoff bei 700 °C in einer Platinapparatur behandelt. Actinium(III)-chlorid (AcCl3) wird durch Umsetzung von Actiniumhydroxid oder -oxalat mit Tetrachlormethan bei Temperaturen oberhalb von 960 °C erhalten. Die Reaktion von Aluminiumbromid und Actinium(III)-oxid führt zum Actinium(III)-bromid (AcBr3) und Behandlung mit feuchtem Ammoniak bei 500 °C führt zum Oxibromid AcOBr. Gibt man Natriumdihydrogenphosphat (NaH2PO4) zu einer Lösung von Actinium in Salzsäure, erhält man weiß gefärbtes Actiniumphosphat (AcPO4 · 0,5 H2O); ein Erhitzen von Actinium(III)-oxalat mit Schwefelwasserstoff bei 1400 °C für ein paar Minuten führt zu schwarzem Actinium(III)-sulfid (Ac2S3).
Ang Lee wurde 1954 in Taiwan geboren. Seine Eltern, Emigranten aus China, lernten sich in Taiwan kennen, Lee ist ihr ältester Sohn. Die Großeltern väterlicher- und mütterlicherseits sind im Zuge der kommunistischen Revolution in China ums Leben gekommen. Da sein Vater als Lehrer häufiger die Arbeitsstelle wechselte, wuchs Ang Lee in verschiedenen Städten Taiwans auf. Entgegen den Wünschen seiner Eltern, wie sein Vater eine klassische akademische Laufbahn einzuschlagen, interessierte sich Lee für das Schauspiel und absolvierte mit ihrem Einverständnis zunächst ein Theater- und Filmstudium in Taipeh. Im Anschluss daran ging er 1978 in die USA, um an der Universität von Illinois in Urbana-Champaign Theaterwissenschaft und -regie zu studieren. Nach dem Erwerb seines B.A. in Illinois verlegte er sich ganz auf das Studium der Film- und Theaterproduktion an der Universität von New York, das er 1985 mit einem Master abschloss. Danach entschloss er sich, mit seiner ebenfalls aus Taiwan stammenden Ehefrau zusammen in den USA zu bleiben. Sein Interesse verschob sich trotz erster Erfahrungen mit dem Super-8-Film in Taiwan erst spät ganz auf Filmregie und -produktion – auch weil Lee seinen Berufswunsch seiner Familie und insbesondere seinem Vater gegenüber lange Zeit nicht eingestehen wollte. Nach dem Studium konnte er zunächst keine eigenen Projekte umsetzen. Erst ab 1992, als er seinen ersten Langfilm fertigstellte, zeichnete sich eine kontinuierliche Karriere als Regisseur ab. Als seine bisher größte Erfolge – sowohl beim Publikum als auch bei der Kritik – gelten das Martial Arts-Drama ''Tiger and Dragon'' mit einer pan-asiatischen Starbesetzung und der Post-Western-Liebesfilm ''Brokeback Mountain'' mit Heath Ledger und Jake Gyllenhaal. Für Letzteren bekam Lee 2006 als erster asiatisch-stämmiger und nicht-weißer Regisseur den Oscar für die beste Regie. Außerdem wurden Lees Filme, neben vielen weiteren Preisen, mit mittlerweile zwei Goldenen Bären der Berlinale und zwei Goldenen Löwen der Filmfestspiele von Venedig ausgezeichnet. Lee ist seit 1983 mit der Mikrobiologin Jane Lin verheiratet. Sie leben in White Plains, Westchester County, im Bundesstaat New York. Aus der Ehe stammen die Söhne Haan (* 1984) und Mason (* 1990).  Ang Lee besitzt die US-amerikanische Staatsbürgerschaft. Nach seinen ersten Filmerfahrungen in Taiwan setzte sich Lee erst wieder während seines Studiums in den USA ernsthaft mit dem Filmemachen auseinander. Im Rahmen seines Studiums in New York drehte er einige Kurzfilme und wirkte unter anderem beim Abschlussdreh seines Studienkollegen Spike Lee als Regieassistent mit. Sein eigener Abschlussfilm ''Fine Line'' gewann 1985 zwei Preise beim renommierten Filmfest seiner Universität. Erst 1992 gelang es ihm, nach dem Gewinn eines hochdotierten Drehbuchwettbewerbs in Taiwan, den ersten einer Reihe von drei Filmen zu drehen, die west-östliche Konflikte taiwanischer Familien zum Thema haben. Diese ersten drei Langfilme, die Lee realisieren konnte, werden im Allgemeinen unter dem Begriff ''Father Knows Best'' gefasst. Diese Bezeichnung geht auf die wiederkehrende Figur des chinesischen Familienoberhaupts, gespielt jeweils vom taiwanischen Schauspieler Sihung Lung, zurück. Die drei Filme thematisieren, wie später noch öfter bei Ang Lee, familiäre Probleme, die aus dem Konflikt zwischen Selbstbestimmung und Tradition, zwischen Innen und Außen, zwischen Ost und West sowie zwischen den Generationen herrühren. Die Filme sind allesamt US-amerikanisch-taiwanische Koproduktionen. Anders als bei allen bislang folgenden Projekten handelt es sich bei den ersten Filmen Lees nicht um Adaptionen, sondern um Filme nach von ihm selbst geschriebenen Originaldrehbüchern. Der erste Film, ''Schiebende Hände'' (1992), handelt vom Einzug eines chinesischen Vaters bei seinem erwachsenen Sohn und der US-amerikanischen Schwiegertochter in New York und den interkulturellen Problemen, die in der neuen Wohngemeinschaft entstehen. Dies war die erste Zusammenarbeit zwischen Lee und dem Drehbuchautor und Produzenten James Schamus – seitdem bildeten die beiden bei jedem Film Lees eine enge Arbeitsgemeinschaft. Wie in den beiden folgenden Filmen schrieben sie auch gemeinsam das Drehbuch. In allen weiteren Filmen Lees (mit Ausnahme des Kurzfilms ''The Hire: Chosen'') hat Schamus seither entscheidende Funktionen ausgeübt. Auch die regelmäßige Zusammenarbeit mit dem Filmeditor Tim Squyres nahm in Lees Erstling ihren Anfang. Mit Ausnahme des Erfolgsfilms ''Brokeback Mountain'' von 2005 hat Squires jeden Film, den Ang Lee gedreht hat, geschnitten. Nach dem Erfolg seines Erstlings konnte Lee als Nächstes ''Das Hochzeitsbankett'' (1993) drehen, eine Komödie über die fingierte Eheschließung eines homosexuellen Exil-Taiwaners in den USA. Erneut taucht hier die Figur des strengen, aber weisen Familienoberhaupts auf. Hatte ''Schiebende Hände'' zunächst vor allem in Taiwan für Aufmerksamkeit (und Preise) gesorgt, wurde mit dem zweiten Langfilm Lees auch Europa auf den aufstrebenden Regisseur aufmerksam: Der Film erhielt bei der Berlinale 1993 den ''Goldenen Bären'' als ''Bester fremdsprachiger Film'' und war zudem für einen Oscar nominiert. Er gilt darüber hinaus als einer der profitabelsten Low-Budget-Filme des Jahres 1993. Mit nur einer Million US-Dollar Produktionskosten erzielte er ein Einspielergebnis von über 23 Millionen US-Dollar. Sihung Lung ist auch im letzten Teil der Trilogie, ''Eat Drink Man Woman'' (1994), die „kongeniale Verkörperung des chinesischen Familienoberhaupts“, das „Zentrum dieser Maskeraden, in denen es darum geht, ein altes Gesicht zu wahren und dann zu lernen, es zu verlieren, um ein neues, lebenstauglicheres zu gewinnen.“ Dieses Mal ist er der verwitwete Vater dreier Töchter, die ihr Leben und ihre Lieben auf unterschiedliche Art angehen und dabei ebenfalls innerfamiliäre Konflikte klären müssen. ''Eat Drink Man Woman'' wurde, anders als seine Vorgänger, in Taipeh gedreht. Im Mittelpunkt des Films stehen (der Titel deutet es an) die Liebe und das Essen. Ang Lee, privat ein passionierter Koch, legte hierbei besonders großen Wert auf die kulinarische Komponente als Stilmittel und konzipierte die Hauptfigur des älteren Witwers als berühmten Koch. Mit dem Angebot der Produzentin Lindsay Doran, die von der britischen Schauspielerin Emma Thompson verfasste Adaption des Romans ''Verstand und Gefühl'' von Jane Austen in Großbritannien zu drehen, eröffnete sich Lee eine lange ersehnte neue Perspektive jenseits asiatisch geprägter Stoffe. In einer neuen Trilogie setzt er sich mit unterschiedlichen Kulturen auseinander: Für ''Brokeback Mountain'' wurde Lee mit einer Vielzahl von Filmpreisen geehrt, darunter der Oscar für die beste Regie, der Goldene Löwe der Filmfestspiele von Venedig sowie die Auszeichnung der Hollywood Foreign Press Association als bester Regisseur des Jahres. 2007 verfilmte er mit ''Gefahr und Begierde'' eine Kurzgeschichte von Eileen Chang. Der Thriller spielt zur Zeit des Zweiten Weltkriegs in Shanghai und handelt von einer jungen chinesischen Agentin (gespielt von Tang Wei), die beauftragt wird, einen hochrangigen Verräter (Tony Leung Chiu Wai) zu liquidieren. Lees erste chinesischsprachige Spielfilmproduktion seit ''Tiger and Dragon'' war 2007 im offiziellen Wettbewerb der 64. Filmfestspiele von Venedig vertreten und brachte ihm erneut den Goldenen Löwen ein. Im selben Jahr wurde ''Gefahr und Begierde'' als offizieller taiwanischer Beitrag für die Nominierung um den besten fremdsprachigen Film bei der Oscar-Verleihung 2008 ausgewählt, später aber auf Empfehlung der Academy of Motion Picture Arts and Sciences wieder zurückgezogen und durch Chen Huai-Ens ''Lian xi qu'' ersetzt. Ende Februar 2009 wurde bekannt gegeben, dass Lee die Jury der 66. Filmfestspiele von Venedig leiten werde. Zwei Monate später erhielt er für seine Komödie ''Taking Woodstock'' eine Einladung in den Wettbewerb der 62. Internationalen Filmfestspiele von Cannes. 2013 wurde er in die Wettbewerbsjury des 66. Filmfestivals von Cannes berufen. Ang Lee ist ein international anerkannter und erfolgreicher Regisseur und gilt als einer der vielseitigsten Filmemacher der letzten Jahre. Häufig behandelt Lee in seinen Filmen das Thema Familie auf eine Art und Weise, die autobiographische Züge seines eigenen Lebens trägt. Er lässt seine Umgebung ganz bewusst auf sich einwirken und bringt diese in seine Filme ein. Kennzeichnend für die meisten seiner Filme ist eine wenig geradlinige Erzählstruktur, die die Charaktere und die Geschichte aus verschiedenen Blickwinkeln darstellt. Er verknüpft die Konflikte des menschlichen Lebens mit traditionellen und innovativen Stilelementen. Für Ang Lee sind die klassisch-soliden Erzählstrukturen zu langweilig, daher kombiniert er verschiedene Genres und Epochen. Er selbst sagte einmal:
Die '''Anschlussfähigkeit''' ist die Kapazität von Systemen zu gewährleisten, dass sich an die Selektionen eines Systems weitere anschließen können. Alle sozialen Systeme reproduzieren sich über Kommunikation (z. B. Wirtschaftssystem oder Politik) oder Handlungen (Medizin und Erziehungssystem). Dies gelingt nur, wenn die einzelnen Einheiten aneinander anschlussfähig sind, was durch einen systemspezifischen Code geleistet wird, der als zentrale Logik (Leitunterscheidung) aller Kommunikation zugrunde liegt und sie als systemzugehörig erkennbar macht. Im Wirtschaftssystem beispielsweise sorgt der Code ''zahlen/nicht zahlen'' dafür, dass die Kommunikationen sich auf sich selbst beziehen und sich selbst reproduzieren können, also dass auf jede Zahlung eine neue erfolgt. Dies funktioniert über das generalisierte Kommunikationsmedium Geld, das die letzte Zahlung mit der jetzigen verknüpft. Würde das Geld nicht mehr akzeptiert, folgt der Zahlung keine weitere Zahlung mehr und das System hätte seine Anschlussfähigkeit verloren. Die Anschlussfähigkeit innerhalb eines Systems wird als Selbstreferenz bezeichnet, im Gegensatz zum fremdreferentiellen Bezug auf die Umwelt (Welt, andere Systeme). Den Begriff hat Luhmann auf eine Anregung eines Bielefelder Kollegen, des Philosophen Jürgen Frese entwickelt. Frese zeigte in einem Sektionsreferat des Achten Deutschen Kongresses für Philosophie in Heidelberg (1966, gedruckt 1967) mit dem Titel „Sprechen als Metapher für Handeln“, dass es fruchtbar ist, von den dominanten Handlungsmodellen Arbeit und Konsum abzurücken und ergänzend Sprechen als Modell für Handeln zu nutzen. Frese schreibt: „Die wichtigste Errungenschaft, die die Sprachmetapher für die Aufhellung des nicht-sprachlichen Handelns einbringt, ist ihre Leistung, Reihenbildung erklärbar zu machen. Fassen wir Satz und Handlung zum neutralen und an andere Philosopheme anschließbaren Begriff des Aktes zusammen, so können wir ... sagen: Der Sinn eines Aktes ist das als eine bestimmte Situation gegebene Ensemble der Möglichkeiten, an diesen Akt weitere Akte anzuschließen; d. h. der Sinn eines Aktes ist die Mannigfaltigkeit der Anschließbarkeiten, die er eröffnet.“ Diese Idee wurde von Luhmann aufgegriffen und im Rahmen seiner Systemtheorie weiterentwickelt. Frese selbst baute sie im Rahmen seiner Lehre von den Formularen weiter aus.
Die '''Aussagenlogik''' ist ein Teilgebiet der Logik, das sich mit Aussagen und deren Verknüpfung durch Junktoren befasst, ausgehend von strukturlosen Elementaraussagen (Atomen), denen ein Wahrheitswert zugeordnet wird. In der ''klassischen Aussagenlogik'' wird jeder Aussage genau einer der zwei Wahrheitswerte „wahr“ und „falsch“ zugeordnet. Der Wahrheitswert einer zusammengesetzten Aussage lässt sich ohne zusätzliche Informationen aus den Wahrheitswerten ihrer Teilaussagen bestimmen. Historisch geht die Aussagenlogik zurück bis zu Aristoteles, der erstmals aussagenlogische Grundsätze diskutierte, nämlich in seiner Metaphysik den Satz vom Widerspruch und den Satz vom ausgeschlossenen Dritten, und der in seiner ersten Analytik den indirekten Beweis thematisierte. Die zweiwertige aussagenlogische Semantik entwickelten etwas später die megarischen Philosophen Diodoros Kronos und Philon. Die Aussagensemantik und -axiomatik kombinierte der Stoiker Chrysippos von Soli, der den ersten aussagenlogischen Kalkül formulierte. Die Weiterentwicklung der Aussagenlogik der Stoa durch das Mittelalter wird oft übersehen. Eine erste vollständige und entscheidbare Formalisierung für aussagenlogische Tautologien – allerdings noch nicht für das aussagenlogische Schließen – schuf George Boole 1847 mit seinem algebraischen Logikkalkül. Den ersten aussagenlogischen Kalkül mit Schlussregeln formulierte Gottlob Frege im Rahmen seiner Begriffsschrift 1879. Er war die Vorlage für den Aussagenkalkül von Bertrand Russell 1908, der sich später durchsetzte (s. u.). Da in der heutigen Mathematik die klassische Aussagenlogik maßgeblich wurde, wird in diesem Artikel dieser moderne Haupttypus der Aussagenlogik behandelt. Allgemein ist die klassische Logik durch zwei Eigenschaften charakterisiert: Das Prinzip der Zweiwertigkeit wird oft mit dem Satz vom ausgeschlossenen Dritten verwechselt. Die ''klassische Aussagenlogik'' ist jenes Gebiet der klassischen Logik, das die innere Struktur von Sätzen (Aussagen) daraufhin untersucht, aus welchen anderen Sätzen (Teilsätzen) sie zusammengesetzt sind und wie diese Teilsätze miteinander verknüpft sind. Die innere Struktur von Sätzen, die ihrerseits nicht in weitere Teilsätze zerlegt werden können, wird von der Aussagenlogik nicht betrachtet. Ein Beispiel: Die Aussage „Alle Katzen sind Hunde, und die Erde ist eine Scheibe“ ist mit dem Bindewort „und“ aus den beiden kürzeren Aussagen „Alle Katzen sind Hunde“ und „Die Erde ist eine Scheibe“ zusammengesetzt. Diese beiden Aussagen lassen sich ihrerseits nicht mehr in weitere Aussagen zerlegen, sind aus aussagenlogischer Sicht also elementar oder atomar. Andere, auf die Aussagenlogik aufbauende logische Systeme betrachten die innere Struktur solcher atomaren Aussagen; ein wichtiges Beispiel ist die Prädikatenlogik. In Abgrenzung zur klassischen Logik entstehen ''nichtklassische Logiksysteme'', wenn man das Prinzip der Zweiwertigkeit, das Prinzip der Extensionalität oder sogar beide Prinzipien aufhebt. Nichtklassische Logiken, die durch die Aufhebung des Prinzips der Zweiwertigkeit entstehen, heißen mehrwertige Logik. Die Zahl der Wahrheitswerte (in diesem Falle üblicher: Pseudowahrheitswerte) kann dabei endlich sein (z. B. dreiwertige Logik), ist aber oft auch unendlich (z. B. Fuzzy-Logik). Hingegen verwenden Logiken, die durch die Aufhebung der Extensionalität entstehen, Junktoren (Konnektive), bei denen sich der Wahrheitswert des zusammengesetzten Satzes nicht mehr eindeutig aus dem Wahrheitswert seiner Teile bestimmen lässt. Ein Beispiel für nichtextensionale Logik ist die Modallogik, die die einstelligen nichtextensionalen Operatoren „es ist notwendig, dass“ und „es ist möglich, dass“ einführt. Logische Systeme stehen innerhalb der Logik nicht in einem Konkurrenzverhältnis um Wahrheit oder Richtigkeit. Die Frage, welches logische System für einen bestimmten Zweck genutzt werden soll, ist eher eine pragmatische. Oft werden logische Systeme und logische Fragestellungen mit außerlogischen Fragen verwechselt oder vermischt, z. B. mit der metaphysischen Frage, welches logische System „richtig“ sei, d. h. die Wirklichkeit beschreibe. Zu dieser Frage gibt es unterschiedliche Standpunkte einschließlich des positivistischen Standpunkts, dass diese Frage sinnlos sei. Diese Fragen fallen aber in andere Gebiete, z. B. Philosophie, Wissenschaftstheorie und Sprachwissenschaft. Wenn in diesem Artikel die klassische Aussagenlogik behandelt wird, so ist das also nicht als metaphysische Festlegung zu verstehen oder gar als Behauptung, dass „alle Aussagen wahr oder falsch sind“. Es ist lediglich so, dass die klassische Aussagenlogik einfach nur solche Aussagen behandelt, die wahr oder falsch sind. Das ist eine große formale Vereinfachung, die dieses System relativ leicht erlernbar sein lässt. Braucht man aus metaphysischen oder pragmatischen Gründen mehr als zwei Wahrheitswerte, kann die klassische Aussagenlogik als Ausgangspunkt dienen, um ein geeignetes logisches System aufzustellen. Eine Aussage A ist ein Satz, der entweder wahr (w, wahr, true, 1) oder nicht wahr (f, falsch, false, 0) ist. Das gilt sowohl für einfache als auch für verknüpfte Aussagen. „Halbwahrheiten“ gibt es nicht. Eine Aussage kann sowohl der gewöhnlichen Sprache entstammen als auch der Sprache der Mathematik. Eine Elementaraussage ist eine Aussage, die keine aussagenlogischen Verknüpfungen (''nicht, und, oder, wenn … dann, genau dann wenn'') enthält. Beispiele für Elementaraussagen: herausstellen. In der klassischen Aussagenlogik ist eine Aussage entweder wahr oder nicht wahr, auch wenn man den Wahrheitsgehalt nicht kennt. Das ist zum Beispiel bei den ungelösten mathematischen Problemen der Fall. Die ''Verneinung'' bzw. ''Negation'' (auch: ''Satzverneinung'', ''äußere Verneinung'', ''kontradiktorisches Gegenteil'') einer Aussage A ist diejenige Aussage ¬A, die genau dann wahr ist, wenn A falsch ist, und die genau dann falsch ist, wenn A wahr ist. Einfacher: Die Verneinung einer Aussage A dreht den Wahrheitswert von A in sein Gegenteil um. Man erhält die Verneinung einer Aussage A immer dadurch, dass man ihr die Formulierung „Es ist nicht der Fall, dass“ voranstellt. Zwar lässt sich ein natürlichsprachlicher Satz auch verneinen, indem man das Wort „nicht“ oder eine andere negative Formulierung an geeigneter Stelle einfügt – es ist aber nicht immer ganz einfach, zu erkennen, welche Formulierung zu verwenden und an welcher Stelle einzufügen ist. Formal schreibt man für „nicht A“ in der gebräuchlichsten Notation (Schreibweise) ¬A, auf Englisch und in der Schaltalgebra auch „NOT A“, gelegentlich auch „~A“. Wir verneinen die obigen Beispiele: Allgemein gilt für die Verneinung: Eine ''Konjunktion'' ist eine aus zwei Aussagen zusammengesetzte Aussage, die die Wahrheit all ihrer Teilaussagen behauptet. Umgangssprachlich verbindet man zwei Aussagen A und B durch das Bindewort „und“ zu einer Konjunktion „A und B“, in der logischen Sprache verwendet man meist das Zeichen  (Schreibweise: ), gelegentlich auch das kaufmännische Und, den Ampersand (&). Die Aussage  ist immer dann wahr, wenn sowohl A als auch B jeweils wahr sind. Andernfalls ist  falsch, nämlich dann, wenn entweder A oder B oder beide Aussagen falsch sind. Beispiele für eine ''Und''-Verknüpfung: Diese Teilaussagen und ihre Negationen werden nun durch  miteinander verknüpft: Nur  ist wahr, weil  wahr ist und auch  wahr ist. ist. Das Formelzeichen „“ stammt von dem lateinischen Wort „vel“, was auf deutsch „oder“ bedeutet. Die Aussage   ist immer dann wahr, wenn mindestens eine der Teilaussagen A oder B wahr ist, bzw. wenn beide Teilaussagen wahr sind. Andernfalls ist   falsch, nämlich dann, wenn sowohl A als auch B falsch sind. Beispiel für eine ''Oder''-Verknüpfung: Diese Teilaussagen und ihre Negationen werden nun durch  miteinander verknüpft: Nur  ist falsch, weil sowohl  als auch  falsch sind. In einem Konditional nennt man A das ''Antezedens'', B das ''Konsequens'' oder ''Sukzedens''. Beispiele: Die Lesart „wenn … dann“ ist insofern problematisch, als mit dem natürlichsprachlichen „wenn … dann“ vor allem inhaltliche Zusammenhänge wie Kausalität oder zeitliche Nähe ausgedrückt werden. All das macht die materiale Implikation nicht, sie nennt nur den formalen Zusammenhang: „Dass es regnet, ist eine hinreichende Bedingung dafür, dass die Straße nass ist“. Zur Frage, ''warum'' das eine hinreichende Bedingung ist – ob auf Grund eines kausalen Zusammenhangs oder auch nur rein zufällig –, nimmt die materiale Implikation nicht Stellung. Als ''Umkehrschluss'' bezeichnet man den Schluss von  auf . Für die Beispiele bedeutet das: verleiten: Die Implikation ist ein wichtiges Mittel in der Mathematik. Die meisten mathematischen Beweise verwenden das Konzept der Implikation. Das ''Bikonditional'', oft auch ''objektsprachliche Äquivalenz'' oder ''materiale Äquivalenz'' genannt, drückt die ''hinreichende und notwendige Bedingung'' aus, sagt also, dass eine Aussage A genau dann zutrifft, wenn eine Aussage B zutrifft. Man schreibt: Auch beim Bikonditional wird eine rein formale Aussage getroffen, die nichts über einen allfälligen inhaltlichen Zusammenhang von A und B aussagt. Statt  zu sagen, kann man auch sagen, dass A eine hinreichende Bedingung für B und dass B eine hinreichende Bedingung für A ist, also . Tatsächlich sind diese beiden Aussagen logisch äquivalent. Beispiel: Das Bikonditional als zusammengesetzte Aussage innerhalb der logischen Sprache (siehe Objektsprache) wird oft mit dem Konzept der logischen Äquivalenz verwechselt oder vermischt. Die logische Äquivalenz ist eine metasprachliche, meist natürlichsprachlich formulierte Eigenschaft zweier Aussagen der logischen Sprache. Ein Zusammenhang zwischen logischer Äquivalenz und Bikonditional besteht nur insofern, als das Metatheorem gilt, dass ein Bikonditional  genau dann eine Tautologie ist, wenn die beiden Aussagen A und B logisch äquivalent sind. Obwohl das ausschließende Oder ein Konzept ist, mit dem man in der natürlichen Sprache immer wieder zu tun hat, wird es in den meisten logischen Sprachen nicht als eigenständiger Junktor eingeführt. Stattdessen wird das ausschließende Oder zum Beispiel als verneintes Bikonditional ausgedrückt, also als . Große Bedeutung genießt das ausschließende Oder hingegen in der Schaltalgebra, wo es meist als XOR ''(eXclusive OR)'' aufgeschrieben wird. Die Verneinung der Konjunktion „A und B“ (in der logischen Schreibweise: ) lautet „Es ist nicht der Fall, dass A und B zutreffen“ (in der logischen Schreibweise: ). Ein Beispiel: In der Schaltalgebra wird sehr oft der Junktor NAND verwendet, wobei „A NAND B“ denselben Wahrheitswertverlauf hat wie der Ausdruck . Die Verneinung der Disjunktion „A oder B (oder beides)“ (in der logischen Schreibweise: ) lautet „Es ist nicht der Fall, dass A oder B zutrifft“ (in logischer Schreibweise: ). Ein Beispiel: Nach dem Gesetz von De Morgan kann man nun aber auch sagen: In der Schaltalgebra wird das Konnektiv NOR verwendet, das denselben Wahrheitswertverlauf hat wie die Aussage . Dieser Abschnitt soll den zunächst oft als kontraintuitiv empfundenen Zusammenhang zwischen hinreichender und notwendiger Bedingung, wie er im Abschnitt über die materiale Implikation angesprochen wurde, wiederaufgreifen und näher ausführen. Betrachten wir noch einmal die materiale Implikation . Man sagt: A ist ''hinreichend'' für B: Schon wenn A der Fall ist, ist auch B der Fall. Umgekehrt kann man aber auch sagen: B ist ''notwendig'' für A. Ohne B kann A nicht erfüllt sein. Wie kommt dieser Zusammenhang zustande? Wir wissen, dass die Wahrheit von A die Wahrheit von B nach sich zieht, denn A ist ja hinreichende Bedingung für B. Somit ist es einfach nicht möglich, dass A eintritt, ohne dass B damit ebenfalls eintreten würde: B ist also gezwungenermaßen der Fall, wenn A der Fall ist. B ist „notwendig“ für A. Dieser Zusammenhang ist in Wahrheit also ziemlich einfach; Hauptgrund dafür, dass er anfangs oft als kontraintuitiv empfunden wird, ist wahrscheinlich die Schwierigkeit, zwischen den vielen Bedeutungen des umgangssprachlichen „wenn … dann“ einerseits und der rein formalen hinreichenden und notwendigen Bedingung andererseits strikt zu trennen. All dies sagt die materiale Implikation aber nicht aus. „A ist eine hinreichende Bedingung für B“ meint schlicht, dass wenn die Aussage A wahr ist, auch die Aussage B wahr ist – zeitlos und zusammenhanglos, nicht etwa „später“ oder „weil“. Analog sagt die notwendige Bedingung, „B ist eine notwendige Bedingung für A“, lediglich das aus, dass B wahr ist, sofern A es ist. Genau das ist aber die Definition des Konditionals A → B. Spätestens beim ''lauten'' Lesen von Sätzen wie: wird der selbstbewusste Laie verlangen, dass ihm erklärt wird, was das soll. Die Antwort des Logikers: Es soll versucht werden, Sicherheit in die Regeln des logischen Schließens zu bringen. Seit den Sophisten ist dem Abendland klar, dass scheinbar zwingende Schlüsse zu offensichtlich absurden Ergebnissen führen können. Immer wieder wurden Paradoxien formuliert und von großen Denkern als Herausforderung empfunden. Logiker versuchen deshalb, die Regeln des Argumentierens so streng wie möglich zu fassen. Das einleitende Beispiel macht klar, dass dazu eine ''Trennung der Sprachebenen'' unerlässlich ist: Die formale Aussage A∧B soll dadurch erklärt werden, dass auf einer metasprachlichen Ebene über die Aussage A wie auch über die Aussage B geredet wird. Das Ziel dabei ist einerseits, dass in einem formalen System nur Zeichenketten (Sätze) hergeleitet werden können, die bei einer plausiblen Interpretation auch wahr sind. Andererseits sollen alle Sätze, die als „wahr“ interpretierbar sind, auch hergeleitet werden können. Das erste ist die Forderung nach ''Korrektheit'', das zweite die nach ''Vollständigkeit'' des formalen Systems; beide Eigenschaften sind unter Kalkül: Der Begriff Kalkül in der Logik beschrieben. Für die klassische Aussagenlogik, mit der wir es hier zu tun haben, gibt es Kalküle (formale Systeme), die sowohl korrekt als auch vollständig sind. Für komplexere logische Systeme (z. B. Mengenlehre) ist es aber ''unmöglich'', einen vollständigen Kalkül aufzustellen, der auch korrekt ist – diese Erkenntnis wurde 1931 von Kurt Gödel bewiesen (Gödelscher Unvollständigkeitssatz). Es gibt viele verschiedene Möglichkeiten, die Syntax („Grammatik“) einer logischen Sprache formal zu definieren; meist geschieht das im Rahmen eines Kalküls. Die folgende Definition ist daher nur als Beispiel dafür zu verstehen, wie ein Kalkül für die klassische Aussagenlogik aussehen kann. Weitere Beispiele für konkrete Kalküle finden sich unter Baumkalkül, Begriffsschrift, Systeme natürlichen Schließens, Sequenzenkalkül oder Resolutionskalkül. Ein weiterer axiomatischer Kalkül ist als Beispiel im Artikel Hilbert-Kalkül angegeben, ein graphischer Kalkül im Artikel Existential Graphs. Als ''Bausteine'' der aussagenlogischen Sprache sollen ''Satzbuchstaben'' („atomare Formeln“, Satzkonstanten), ''Junktoren'' und ''Gliederungszeichen'' verwendet werden. Satzbuchstaben sollen die Zeichen P0, P1, P2, … sein. Junktoren sollen die Zeichen ¬, ∧, ∨, → und ↔ sein. Als Gliederungszeichen sollen die runden Klammern dienen. Formal lässt sich das z. B. auf folgende Weise ausdrücken: Sei V die (abzählbar unendliche) Menge der ''atomaren Formeln'' (Satzbuchstaben): Sei J die Menge der Junktoren und Gliederungszeichen: Das ''Alphabet'' der logischen Sprache sei die Menge V ∪ J, also die Vereinigungsmenge von atomaren Formeln, Junktoren und Gliederungszeichen. Die ''Formationsregeln'' legen fest, wie man aus den Bausteinen der aussagenlogischen Sprache Sätze (Formeln) bilden kann. Hier sollen ''aussagenlogische Formeln'' als Worte über dem Alphabet der logischen Sprache, also über V ∪ J wie folgt induktiv definiert werden: Innerhalb der Syntax sind die Schlussregeln allerdings rein formale Transformationsregeln, denen für sich keinerlei inhaltliche Bedeutung zukommt. An konkreten Schlussregeln sollen hier nur zwei angegeben werden: Der Modus ponendo ponens und die Substitutionsregel. Pragmatisch wählt man solche Formeln als Axiome, die semantisch gesehen Tautologien sind, also immer zutreffen, und die dabei helfen, Beweise zu verkürzen. Innerhalb der Syntax sind die Axiome allerdings rein formale Objekte, denen keinerlei inhaltliche Bedeutung oder Rechtfertigung zukommt. Axiome sind im Allgemeinen optional, d. h. ein Kalkül kann auch ganz ohne Axiome auskommen, wenn er ausreichend viele bzw. mächtige Schlussregeln hat. Axiomfreie Kalküle sind zum Beispiel die Systeme natürlichen Schließens oder Baumkalküle. Hier soll exemplarisch ein axiomatischer Kalkül gezeigt werden, und zwar Russells Aussagenkalkül aus seiner Typentheorie 1908, den er 1910 in die Principia Mathematica übernahm. Dieser Kalkül umfasst die folgenden Axiome (von denen das vierte redundant, d. h. nicht unbedingt erforderlich, weil aus den anderen Axiomen herleitbar ist): Um aus diesen Axiomen auch solche gültigen Sätze herleiten zu können, die andere als die in den Axiomen vorkommende Junktoren enthalten, werden diese durch folgende Festlegung auf die vorhandenen Junktoren zurückgeführt: Alternativ zu – wie hier – konkreten Axiomen kann man auch ''Axiomenschemata'' angeben, in welchem Fall man auch ohne Substitutionsregel auskommt. Interpretiert man die obigen Axiome als Axiomenschemata, dann stünde z. B. das erste Axiomenschema, , für unendlich viele Axiome, nämlich alle Ersetzungsinstanzen dieses Schemas. Eine Herleitung ist eine Liste von aufsteigend nummerierten Sätzen, die mit einer oder mehreren Annahmen (den Prämissen der Herleitung) oder Axiomen beginnt. Alle auf diese folgenden Sätze sind entweder ebenfalls Axiome (bei manchen Kalkülen sind auch weitere Annahmen zulässig) oder sind aus einer oder mehreren der vorangehenden Zeilen durch Anwendung von Schlussregeln entstanden. Der letzte Satz in der Liste ist die Konklusion der Herleitung. Eine Herleitung ohne Prämissen heißt ''Beweis''. Oft werden aber die Wörter „Herleitung“ und „Beweis“ synonym gebraucht. Wenn es gelingt, aus einer Menge von Annahmen (Prämissen) Δ eine Konklusion P herzuleiten, dann schreibt man auch . Gelingt es, einen Satz P ohne die Verwendung von Annahmen herzuleiten (zu beweisen), dann schreibt man auch: . In diesem Fall wird P ''Theorem'' genannt. Das Zeichen  geht auf die Begriffsschrift zurück, jenes Werk, in dem Gottlob Frege 1879 die erste Formalisierung der Prädikatenlogik angegeben hat. In der klassischen Aussagenlogik wählt man die Schlussregeln so, dass sich mit ihrer Hilfe ''alle'' gültigen Argumente (und ''nur'' gültige Argumente) herleiten lassen; die Frage der Gültigkeit wird im folgenden Abschnitt, „Semantik“, behandelt. Außerhalb der Logik bezeichnet Semantik ein Forschungsgebiet, das sich mit der Bedeutung von Sprache und deren Teilen befasst. Oft wird auch das Wort ''Semantik'' gleichbedeutend mit dem Wort ''Bedeutung'' verwendet. Auch innerhalb der Logik geht es bei Semantik um Bedeutung: Darum nämlich, den Ausdrücken einer formalen Sprache – zum Beispiel der hier behandelten Sprache der Aussagenlogik – eine Bedeutung zuzuordnen. In der Logik wird auch das meist sehr formal unternommen. Im Zentrum der (formalen) Semantik steht eine Auswertungsfunktion (andere Bezeichnungen lauten Bewertungsfunktion, Denotationsfunktion, Wahrheitswertefunktion), die den Formeln der logischen Sprache eine Bedeutung zuordnet. Formal gesprochen ist die Auswertungsfunktion eine Abbildung von der Menge der Formeln der Sprache in die Menge der Wahrheitswerte. Oft wird die Auswertungsfunktion mit dem Großbuchstaben V bezeichnet. In der klassischen Aussagenlogik ist die Auswertungsfunktion sehr einfach: Das Prinzip der Zweiwertigkeit fordert, dass sie für jede zu bewertende Formel genau einen von genau zwei Wahrheitswerten liefern muss; und das Prinzip der Extensionalität fordert, dass die Bewertungsfunktion beim Bewerten eines komplexen Satzes nur die Bewertung von dessen Teilsätzen berücksichtigen muss. Jedem Atom, also jedem Satzbuchstaben (Atom) wird durch Festsetzung ein Wahrheitswert zugeordnet. Man sagt: Die Atome werden interpretiert. Es wird also z. B. festgelegt dass P0 wahr ist, dass P1 falsch ist und dass P2 ebenfalls falsch ist. Damit ist der Bewertung der Bausteine der logischen Sprache Genüge getan. Formal ist eine solche Bewertung – ''Interpretation'' genannt und oft mit dem Kleinbuchstaben v bezeichnet – eine Funktion im mathematischen Sinn, d. h. eine Abbildung von der Menge der Atome in die Menge der Wahrheitswerte. Wenn die Auswertungsfunktion V auf ein Atom angewandt wird, d. h. wenn sie ein Atom bewerten soll, liefert sie die Interpretation dieses Atoms im Sinn des obigen Absatzes. Mit anderen Worten, sie liefert den Wert, den die Bewertung v dem Atom zuordnet. Um die zusammengesetzten Formeln bewerten zu können, muss für jeden Junktor definiert werden, welchen Wahrheitswert die Bewertungsfunktion für die unterschiedlichen Wahrheitswertkombinationen liefert, den seine Argumente annehmen können. In der klassischen Aussagenlogik geschieht das meist mittels Wahrheitstabellen, weil es nur überschaubar wenige Möglichkeiten gibt. Der einstellige Junktor ¬, die Negation, ist in der klassischen Aussagenlogik so definiert, dass er den Wahrheitswert seines Arguments ins Gegenteil umkehrt, also „verneint“: Ist die Bewertung einer Formel X wahr, dann liefert die Bewertungsfunktion für ¬X falsch; wird aber X falsch bewertet, dann liefert die Bewertungsfunktion für ¬X wahr. Die Wahrheitstabelle sieht folgendermaßen aus: Die Wahrheitswertverläufe der verwendeten zweistelligen Konnektive sind in der klassischen Aussagenlogik wie folgt definiert: Allgemein gibt es für die klassische Aussagenlogik vier einstellige und sechzehn zweistellige Junktoren. Die hier behandelte logische Sprache beschränkt sich nur deshalb auf die Junktoren ¬, ∧, ∨, → und ↔, weil diese am gebräuchlichsten sind und weil sie auch inhaltlich noch am ehesten aus der Alltagssprache bekannt sind. Aus formaler Sicht ist die einzige Bedingung, die man bei der Wahl von Junktoren erfüllen möchte, die, dass sich mit den gewählten Junktoren auch alle anderen theoretisch möglichen Junktoren ausdrücken lassen; man sagt: Dass die Menge der gewählten Junktoren funktional vollständig ist. Diese Anforderung ist bei der hier getroffenen Wahl erfüllt. Näheres zur Frage, wie viele und welche Junktoren es gibt und wie viele Junktoren man benötigt, um funktionale Vollständigkeit zu erreichen, ist im Kapitel Junktor beschrieben. Eine ''Formel'' der aussagenlogischen Sprache heißt genau dann semantisch gültig, wenn die Formel unter allen Interpretationen – d. h. unter allen Zuordnungen von Wahrheitswerten zu den in ihr vorkommenden Atomen – wahr ist; wenn sie sozusagen allgemeingültig ist; mit anderen Worten: Wenn die Wahrheitstabelle für diese Aussage in jeder Zeile das Ergebnis ''wahr'' zeigt. Man nennt semantisch gültige Formeln auch Tautologien und schreibt, wenn  eine Tautologie ist, formal wie folgt: Ein ''Argument'' heißt genau dann semantisch gültig, wenn unter der Voraussetzung, dass alle Prämissen wahr sind, auch die Konklusion wahr ist. In der Formulierung von Gottfried Wilhelm Leibniz: ''Aus Wahrem folgt nur Wahres.'' Diese Definition muss natürlich ebenfalls formal gefasst werden, und das geschieht wie folgt: Ein Argument ist genau dann semantisch gültig, wenn alle Zuordnungen von Wahrheitswerten zu den in Prämissen und Konklusion vorkommenden Atomen, unter denen die Bewertungsfunktion für alle Prämissen den Wert ''wahr'' liefert, auch für die Konklusion den Wert ''wahr'' liefert. Um auszudrücken, dass aus einer Menge  von Formeln (der Prämissenmenge) eine Formel  (die Konklusion) semantisch folgt, schreibt man formal wie folgt: Beachte die graphische Ähnlichkeit und die inhaltliche Verschiedenheit zwischen  (Kapitel „Herleitung und Beweis“) und  (''Siehe:'' Semantische Folgerung): Die erste Formulierung –  – drückt die ''syntaktische'' Gültigkeit des Arguments aus, sagt also, dass aus den Formeln in  mit den Schlussregeln des gewählten Kalküls die Formel  ''hergeleitet'' werden kann.  hingegen behauptet die ''semantische'' Gültigkeit, die in der klassischen Aussagenlogik wie in den vorangegangenen Absätzen als das Leibniz’sche ''Aus Wahrem folgt nur Wahres'' definiert ist. Neben der Eigenschaft der Gültigkeit (Allgemeingültigkeit) gibt es einige andere wichtige Eigenschaften: Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit. Im Gegensatz zur Gültigkeit, die Eigenschaft von Formeln oder von Argumenten sein kann, sind Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit Eigenschaften von Sätzen oder von Satzmengen. Die Frage, ob eine Formel (oder eine Formelmenge) eine der genannten Eigenschaften hat, ist ebenso wie die Frage, ob eine Formel allgemeingültig, d. h. eine Tautologie ist, für allgemeine Formeln nicht effizient lösbar: Zwar ist die Wahrheitstafel ein Entscheidungsverfahren für jede dieser Fragen, doch umfasst eine Wahrheitstafel für eine Aussage bzw. eine Aussagemenge in n Atomen  Zeilen; das Wahrheitstafelverfahren ist nichts anderes als ein Brute-Force-Verfahren. Jede dieser Fragestellungen kann auf die Frage zurückgeführt werden, ob eine bestimmte Formel erfüllbar ist: Die Frage, ob eine Aussage erfüllbar ist, wird Erfüllbarkeitsproblem oder ''SAT-Problem'' (nach dem englischen Wort für Erfüllbarkeit, ''satisfiability'') genannt. Das SAT-Problem spielt eine wichtige Rolle in der theoretischen Informatik und Komplexitätstheorie. Das Erfüllbarkeitsproblem für allgemeine (beliebige) Formeln ist NP-vollständig, d. h. (unter der Voraussetzung, dass P ungleich NP) nicht in polynomialer Laufzeit lösbar. Für bestimmte echte Teilmengen der Formeln der aussagenlogischen Sprache ist das SAT-Problem dennoch schneller, d. h. in polynomial beschränkter Rechenzeit lösbar. Eine solche Teilmenge sind die Horn-Formeln, das sind Konjunktionen von Disjunktionen, deren Disjunkte verneinte oder unverneinte Atome sind, wobei innerhalb einer solchen Disjunktion allerdings höchstens ein Atom unverneint sein darf. Wenn man die Semantik betrachtet, die hier für die klassische Aussagenlogik aufgestellt wurde, dann erkennt man gewisse Gesetzmäßigkeiten. Wird z. B. die Auswertungsfunktion auf eine Aussage der Form X ∧ W angewendet, wobei W eine beliebige wahre Aussage sein soll, dann stellt man fest, dass die Auswertungsfunktion für X ∧ W immer den Wahrheitswert ''wahr'' liefert, wenn V(X)=wahr ist (das heißt V(X∧W)=V(X)). Von der Struktur her gleichwertige Gesetzmäßigkeiten gelten auch in anderen Semantiken, auch in solchen, die für ganz andere, nichtlogische Systeme aufgestellt werden. Für die Arithmetik gilt z. B., dass die dortige Bewertungsfunktion (hier VArithmetik genannt) für einen Ausdruck der Form X + Y immer den Wert von X liefert, sofern der Wert von Y null ist: VArithmetik(X+Y)=VArithmetik(X), wenn VArithmetik(Y) = null ist. Eine formale Wissenschaft, die solche strukturellen Gesetzmäßigkeiten untersucht, ist die abstrakte Algebra (meist Teilgebiet der Mathematik, aber auch der Informatik). In der abstrakten Algebra wird zum Beispiel untersucht, für welche Verknüpfungen es ein neutrales Element gibt, d. h. ein Element ''N'', das für eine Verknüpfung ''op'' dazu führt, dass (für beliebiges X) gilt: X ''op'' ''N'' = X. So würde man aus algebraischer Sicht sagen, dass es für die klassische aussagenlogische Konjunktion genau ein neutrales Element gibt, nämlich ''wahr'', und dass es für die Addition in der Arithmetik ebenfalls genau ein neutrales Element gibt, nämlich die Zahl Null. Nur am Rande sei erwähnt, dass es auch für andere Junktoren neutrale Elemente gibt; das neutrale Element für die Disjunktion ist ''falsch'': V(X ∨ F) = V(X), wenn V(F)=falsch ist. Die formale Algebra betrachtet formale Semantiken rein nach ihren strukturellen Eigenschaften. Sind diese identisch, dann besteht zwischen ihnen aus algebraischer Sicht kein Unterschied. Aus algebraischer Sicht, genauer: Aus Sicht der formalen Algebra ist die Semantik für die klassische Aussagenlogik eine zweiwertige Boolesche Algebra. Andere formale Systeme, deren Semantiken jeweils eine Boolesche Algebra bilden, sind die Schaltalgebra und die elementare Mengenlehre. Aus algebraischer Sicht besteht daher zwischen diesen Disziplinen kein Unterschied. disjunktiver Normalform umformen. In der Metatheorie werden die Eigenschaften von logischen Systemen untersucht: Das logische System ist in der Metatheorie der Untersuchungsgegenstand. Eine metatheoretische Fragestellung ist zum Beispiel die, ob in einem Kalkül ein Widerspruch hergeleitet werden kann. Der vorliegende Abschnitt soll einige wichtige metatheoretische Fragestellungen aus dem Blickwinkel der Aussagenlogik betrachten. Ein metatheoretisches Resultat ist zum Beispiel die Feststellung, dass alle korrekten Kalküle auch konsistent sind. Ein anderes metatheoretisches Resultat ist die Feststellung, dass ein konsistenter Kalkül nicht automatisch korrekt sein muss: Es ist ohne weiteres möglich, einen Kalkül aufzustellen, in dem zwar kein Widerspruch hergeleitet werden kann, in dem aber z. B. die nicht allgemeingültige Aussage der Form „A ∨ B“ hergeleitet werden kann. Ein solcher Kalkül wäre aus ersterem Grund konsistent, aus letzterem Grund aber nicht korrekt. Ein weiteres, sehr einfaches Resultat ist die Feststellung, dass ein vollständiger Kalkül nicht automatisch auch korrekt oder nur konsistent sein muss. Das einfachste Beispiel wäre ein Kalkül, in dem ''jede'' Formel der aussagenlogischen Sprache herleitbar ist. Da jede Formel herleitbar ist, sind alle Tautologien herleitbar, die ja Formeln sind: Das macht den Kalkül vollständig. Da aber jede Formel herleitbar ist, ist insbesondere auch die Formel P0 ∧ ¬ P0 und die Formel A ∨ B herleitbar: Ersteres macht den Kalkül inkonsistent, letzteres inkorrekt. Das Ideal, das ein Kalkül erfüllen sollte, ist Korrektheit und Vollständigkeit: Wenn das der Fall ist, dann ist er der ideale Kalkül für ein logisches System, weil er alle semantisch gültigen Sätze (und nur diese) herleiten kann. So sind die beiden Fragen, ob ein konkreter Kalkül korrekt und/oder vollständig ist und ob es für ein bestimmtes logisches System überhaupt möglich ist, einen korrekten und vollständigen Kalkül anzugeben, zwei besonders wichtige metatheoretische Fragestellungen. Die klassische Aussagenlogik, wie sie hier ausgeführt wurde, ist ein formales logisches System. Als solches ist sie eines unter vielen, die aus formaler Sicht gleichwertig nebeneinander stehen und die ganz bestimmte Eigenschaften haben: Die meisten sind konsistent, die meisten sind korrekt, etliche sind vollständig, und einige sind sogar entscheidbar. Aus formaler Sicht stehen die logischen Systeme in keinem Konkurrenzverhalten hinsichtlich Wahrheit oder Richtigkeit. Von formalen, innerlogischen Fragen klar unterschieden sind außerlogische Fragen: Solche nach der Nützlichkeit (Anwendbarkeit) einzelner Systeme für einen bestimmten Zweck und solche nach dem philosophischen, speziell metaphysischen Status einzelner Systeme. Die Nützlichkeitserwägung ist die einfachere, bezüglich deren Meinungsunterschiede weniger tiefgehend bzw. weniger schwerwiegend sind. Klassische Aussagenlogik zum Beispiel bewährt sich in der Beschreibung elektronischer Schaltungen (Schaltalgebra) oder zur Formulierung und Vereinfachung logischer Ausdrücke in Programmiersprachen. Prädikatenlogik wird gerne angewandt, wenn es darum geht, Faktenwissen zu formalisieren und automatisiert Schlüsse daraus zu ziehen, wie das unter anderem im Rahmen der Programmiersprache Prolog geschieht. Fuzzy-Logiken, nonmonotone, mehrwertige und auch parakonsistente Logiken sind hochwillkommen, wenn es darum geht, mit Wissensbeständen umzugehen, in denen Aussagen mit unterschiedlich starkem Gewissheitsgrad oder gar einander widersprechende Aussagen abgelegt werden sollen und dennoch sinnvolle Schlüsse aus dem Gesamtbestand gezogen werden sollen. Auch wenn es je nach Anwendungsfall sehr große Meinungsunterschiede geben kann, welches logisches System besser geeignet ist, ist die Natur des Problems für alle Beteiligten unmittelbar und in gleicher Weise greifbar. Einzelwissenschaftliche Überlegungen und Fragestellungen spielen sich überwiegend in diesem Bereich ab. In den Bereich metaphysischer Fragestellungen fällt auch die Frage, ob es so etwas wie ein ''metaphysisches'' Prinzip der Zweiwertigkeit gebe, ob also Aussagen über die Wirklichkeit durchgehend ins Schema wahr/falsch passen oder nicht. Diese Frage ist unabhängig von der Frage, ob die Beschäftigung mit zwei- oder mehrwertigen Logiken praktisch sinnvoll ist: Selbst wenn ein metaphysisches Prinzip der Zweiwertigkeit herrscht, könnte man anwendungspraktisch mehrwertige Logiken nützen, etwa dazu, epistemische Sachverhalte zu fassen, zum Beispiel aus Aussagen zu schließen, die zwar metaphysisch wahr oder falsch sind, von denen aber nicht oder noch nicht bekannt ist, welches von beidem der Fall ist. Umgekehrt kann man auch dann, wenn ein solches metaphysisches Prinzip nicht gilt, zweiwertige Logik wegen ihrer Einfachheit für solche Anwendungen bevorzugen, bei denen nur mit solchen Sätzen umgegangen werden muss, die tatsächlich wahr oder falsch sind. Die Frage nach einem metaphysischen Prinzip der Zweiwertigkeit ist wie die meisten metaphysischen Fragen nicht endgültig zufriedenstellend beantwortet. Ein früher Einwand gegen ein solches Prinzip, den Aristoteles zur Diskussion stellte, war das Thema der Aussagen über zukünftige Sachverhalte („Morgen wird es regnen“). Wenn Aussagen über Zukünftiges schon heute wahr oder falsch wären, so wird argumentiert, dann müsse die Zukunft bis ins letzte Detail vorbestimmt sein. Ein anderer Einwand, der vorgebracht wird, ist, dass es Aussagen gibt, deren Wahrheit praktisch oder theoretisch nicht festgestellt werden kann – zum Beispiel lässt sich die Wahrheit von „Der Rasen vor dem Weißen Haus bestand am 1. Februar 1870 aus genau 6.120.375,4 Grashalmen“ einfach nicht feststellen. Befürworter eines metaphysischen Zweiwertigkeitsprinzips berufen sich oft auf das Verhalten von Metatheoretikern, also von Mathematikern oder Logikern, die Aussagen ''über'' formale Systeme treffen: Egal wie mehrwertig oder nichtklassisch das untersuchte System ist, die dabei getroffenen Metavermutungen, Metabehauptungen und Metafeststellungen sind immer zweiwertig: Ein Kalkül, auch ein parakonsistenter oder nonmonotoner, wird immer als ''entweder'' konsistent ''oder'' inkonsistent betrachtet, und ein logisches System ist immer ''entweder'' korrekt oder inkorrekt, vollständig oder nicht vollständig, entscheidbar oder unentscheidbar, niemals „ein bisschen“ von beidem. Befürworter deuten das als Hinweis darauf, dass es in der Wirklichkeit tatsächlich eine strenge Unterscheidung nach wahr und falsch gebe oder dass es zumindest sinnvoll ist, eine solche anzunehmen. Eine andere philosophische Fragestellung ist die nach dem metaphysischen Status des Untersuchungsgegenstands der Logik, also danach, was logische Systeme, Kalküle, Wahrheitswerte eigentlich „sind“. Der platonische Standpunkt besteht darin, dass die in der Logik verwendeten Zeichen und Konstrukte eine außerlogische Bedeutung haben, dass sie Namen für real existierende (wenn auch natürlich nicht-physikalische) Gegenstände sind. In diesem Sinn gäbe es so etwas wie ''das Wahre'' und ''das Falsche'', abstrakte Gegenstände, die von den Zeichen „wahr“ und „falsch“ benannt werden. Der Gegenpol zum Platonismus wäre der Nominalismus, der Existenz nur den Zeichen zuspricht, die in der Logik manipuliert werden. Gegenstand der Logik sind Zeichen, und die Tätigkeit der Logiker ist die Manipulation von Zeichen. Die Zeichen bezeichnen aber nichts, so etwas wie das Wahre oder das Falsche gibt es also nicht. Im Grundlagenstreit der Mathematik entspräche der nominalistischen Position die formalistische Richtung. Eine Mittelstellung nähme der philosophische Konstruktivismus ein, demzufolge die Zeichen zwar keine unabhängig existierenden Gegenstände bezeichnen, durch den Umgang mit den Zeichen aber Gegenstände konstruiert werden.
Minghella war der Sohn italienisch-schottischer Eltern, die auf der Isle of Wight eine Fabrik für Eiscreme betrieben. Nach seinem Schulabschluss studierte er an der Universität Hull, wo er eine Zeit lang als Dozent tätig war. 1978 drehte er einen ersten Kurzfilm. Seit 1981 war er als Autor und Story Editor tätig. Er wurde mit Theaterstücken, Rundfunkhörspielen, der Fernsehserie ''Inspector Morse'' und vielen Drehbüchern für Film und Fernsehen bekannt. Er entwickelte die Drehbücher für die 1988 erfolgreich ausgestrahlte Fernsehserie The Storyteller von Muppets-Erfinder Jim Henson. Auch als Produzent war er erfolgreich, darunter für die Filme ''Der stille Amerikaner'', ''Die Dolmetscherin'' und ''Der Vorleser'', für den er 2008 posthum für den Oscar (Kategorie „Bester Film“) nominiert wurde. Gemeinsam mit seinem Freund und Kollegen Sydney Pollack gründete er die Produktionsfirma Mirage Enterprises. Der Regisseur Minghella galt als ein guter Schauspielerführer: Unter seiner Regie brachten es zahlreiche Darsteller zu Oscar-Nominierungen, zwei Schauspielerinnen erhielten die Auszeichnung als „Beste Nebendarstellerin“: Juliette Binoche (''Der englische Patient'') und Renée Zellweger (''Unterwegs nach Cold Mountain''). Gegen Ende seines Lebens kehrte Minghella zu seinen Anfängen im Radio und auf der Bühne zurück: 2006 wurde sein Hörspiel ''Eyes Down Looking'' mit Jude Law zu Ehren von Samuel Beckett auf BBC Radio 3 ausgestrahlt, ein Jahr zuvor hatte seine Inszenierung der Puccini-Oper Madame Butterfly in der English National Opera in London Premiere und wurde auch in der Nationaloper von Vilnius und in der Metropolitan Opera in New York gezeigt. Am Ende des Films ''Abbitte'' von Joe Wright (2007) hat er einen Kurzauftritt als Talkshow-Moderator neben Vanessa Redgrave. Seine letzte Arbeit als Drehbuchautor war das Skript für den Musical-Film ''Nine'' (gemeinsam mit Michael Tolkin). Zu seinen letzten Regiearbeiten zählt der Pilotfilm zur Krimiserie ''Eine Detektivin für Botswana'' (Originaltitel: ), den die BBC fünf Tage nach seinem Tod erstmals ausstrahlte. Minghella war mit der aus Hongkong stammenden Choreographin, Produzentin und Schauspielerin Carolyn Choa (''Wie verrückt und aus tiefstem Herzen'') verheiratet. Der Ehe entstammen zwei Kinder, die in der Filmbranche tätig sind: Tochter Hannah Minghella in der Produktion und Sohn Max Minghella als Schauspieler (''Agora – Die Säulen des Himmels''). Die Tante Edana Minghella und der Onkel Dominic Minghella (u. a. für die deutsche Fernsehserie ''Doktor Martin'') sind Drehbuchautoren. Minghella starb im Alter von 54 Jahren in einem Londoner Krankenhaus an inneren Blutungen infolge der Operation eines Tonsillenkarzinoms und eines Karzinoms im Nacken. 1984 erhielt Minghella den Londoner Kritikerpreis als meistversprechender junger Dramatiker, 1986 den Kritikerpreis für sein Stück ''Made in Bangkok'' als bestes Stück der Saison. 1997 erhielt er für ''Der englische Patient'' den Oscar in der Rubrik ''Beste Regie'', 1999 eine Oscar-Nominierung in der Kategorie „Bestes adaptiertes Drehbuch“ für ''Der talentierte Mr. Ripley'', bei dem er auch Regie führte. 2001 wurde Minghella zum Commander of the British Empire (CBE) ernannt. Von 2003 bis 2007 war er Präsident des British Film Institute. Seit 1997 trägt das Anthony Minghella Theatre auf der Isle of Wight seinen Namen.
Die '''Geschichte des US-amerikanischen Films''' ist ein Kapitel der Filmgeschichte, das gerade wegen der hervorgehobenen Stellung der Vereinigten Staaten als Filmnation sowohl für die Filmkunst als auch für die Ökonomie des Films relevant ist. Weltruhm erlangte Hollywood, ein Stadtteil von Los Angeles, als Zentrum der US-amerikanischen Filmindustrie, weshalb der Name oft auch als Synonym für die gesamte amerikanische Film-Branche steht. Synonym für Hollywoods Filmindustrie wird wiederum der Begriff ''Traumfabrik'' ( ''Dreamfactory'') verwendet. Bis 1912 konzentrierten sich die US-amerikanischen Filmunternehmen auf den inneramerikanischen Filmwettbewerb. Erst danach stieg ihr Einfluss auf dem Weltmarkt. Und zwar so rapide, dass sie bereits 1914, zu Beginn des Ersten Weltkriegs, die Hälfte der Welt-Filmproduktion stellten. Der harte Wettkampf zwischen dem Edison Trust und den von Carl Laemmle angeführten „Independents“ hatte wirksame Instrumente geschaffen, die, am nationalen Konkurrenten erprobt und verfeinert, nun mit zunehmender Härte die internationalen Mitbewerber trafen. Dennoch war die Vormachtstellung Hollywoods längst nicht unangreifbar, erst eine politische Entwicklung verschaffte ihr die nötige Ruhe zur Restrukturierung: Der Krieg in Europa. Die französische Filmproduktion, Hauptkonkurrent der US-Amerikaner, kam mit dem Ausbruch des Krieges sofort und vollständig zum Erliegen, denn Pathé wandelte seine Rohfilm-Fabrik in eine Munitionsfabrik um und seine Studios in Kasernen. Ähnlich, und doch weniger extrem, brach die italienische Produktion beim Kriegseintritt des Landes 1916 ein. Nachdem absehbar war, dass der Krieg sehr lange dauern konnte, bemühten sich die Franzosen, wieder ins Geschäft zu kommen. Die Position, die sie vor Ausbruch des Krieges innehatten, erreichten sie nicht mehr. Zudem beschloss das Deutsche Reich 1916 das generelle Filmeinfuhrverbot, was die europäischen Filmnationen ihres wichtigsten Absatzmarktes beraubte. Auch der Export nach Übersee gestaltete sich zunehmend schwierig, denn die Militärs beanspruchten viele Transportkapazitäten für sich. Außerdem führten deutsche U-Boote und kleinere Kreuzer einen Handelskrieg gegen die Entente-Mächte, wobei auch zivile Frachter versenkt wurden, da man die Entente verdächtigte, sie für Waffenlieferungen zu missbrauchen (z. B. die Versenkung der RMS Lusitania). Die Macht der Motion Picture Patents Company (MPPC) war 1914 bereits weitgehend gebrochen, die später folgenden Gerichtsurteile waren nur noch Formalitäten. Sowohl die nationale als auch die internationale Konkurrenz der Independents waren also ausgeschaltet. Die US-Filmwirtschaft verlor zwar einen Teil des europäischen Absatzmarktes, doch der Bedarf an frischen Filmen innerhalb der Vereinigten Staaten war höher als in ganz Europa zusammen, so gab es beispielsweise 1916 bereits ca. 28.000 Kinos in ganz Amerika. Auch in der übrigen Welt nahmen die Hollywood-Unternehmen eine dominierende Stellung ein, sie stellten zum Beispiel einen Großteil der in Australien und Südamerika gezeigten Filme, die ab ca. 1916 direkt vertrieben wurden (früher war es üblich, an lokale Zwischenhändler zu verkaufen). Nach Robert C. Allen und Douglas Gomery basiert der freie Wettbewerb zwischen Unternehmen auf vier Punkten: Der erste Versuch, den freien Wettbewerb zu zerstören und ein Oligopol zu bilden, wurde mittels der Patente betrieben. MPPC versuchte, den Zugang fremder Unternehmen zu behindern, indem sie diesen durch Lizenzgebühren den Wettbewerb erschwerte. Um das System durchzusetzen, sollte zudem eine hohe Marktdurchdringung erfolgen. Auf ihrem Höhepunkt kontrollierte die MPPC via Lizenz den Großteil der Kinos. Auch der Zugang zu Filmmaterial war nicht ohne Lizenz möglich, da Eastman Kodak einen Exklusivvertrag mit der MPPC geschlossen hatte. Der Edison-Trust attackierte also vor allem die Punkte 2–4. Das System scheiterte endgültig mit der Annullierung der Edison-Patente durch den Obersten Gerichtshof der Vereinigten Staaten, sein Niedergang jedoch hatte schon wesentlich früher begonnen. Den freien Zugang zum Filmmaterial erlangten die Independents durch den Bau eigener Kameras und durch die Aufhebung des Patents auf Rohfilme 1912. Und um mit dem Trust konkurrieren zu können, begannen sie, ihre Filme von denen der MPPC unterscheidbar zu machen. Hierbei entstanden der Feature Film und das „Starsystem“. Die MPPC war zwar nicht blind gegenüber diesen Neuerungen, auch sie drehte Feature Films, durch ihre Struktur und vor allem durch ihre Kundenstruktur, war sie dennoch nicht in der Lage, mit diesen neuen Instrumenten zu experimentieren. Der Trust wollte Massenware verkaufen um eine bestimmte Marge zu erwirtschaften. Teure Stars hätten nur die Kosten hochgetrieben, und Feature Films bargen ein nicht zu unterschätzendes Risiko, für das die Kunden des Trusts nicht aufkommen wollten. So konnten die „Independents“ den ersten Punkt des freien Wettbewerbs unterhöhlen und einzigartige Filmerlebnisse statt austauschbarer Produkte bieten, was dem Publikumsinteresse deutlich entgegenkam und vor allem finanzkräftigere Mittelschichten erschloss. Der Feature Film kommt ca. 1909 auf und wird nur von den Independents ernsthaft weiterentwickelt, beispielsweise von Famous Players, die später nur noch Features produzieren. Famous Players sind auch die erste Gesellschaft, die das Starsystem konsequent nutzt, nach früheren Versuchen, z. B. von I.M.P. Durch die oben genannten Schritte schaffen es die Independents, sich eine Position im Markt zu sichern und immer weiter auszubauen. Für nationales und internationales Wachstum fehlen ihnen effiziente Strukturen, zum Beispiel in der Distribution. Noch bis in die Mitte der 1910er Jahre hält sich das alte States-Rights-System, in dem der Produzent lokale Franchise-Rechte seines Films an einen Distributor verkauft, der diese dann innerhalb seines festgelegten Gebiets an Kinos weiter verleiht. Diese Situation ändert sich erstmals 1914 mit der Fusion von elf regionalen Distributoren zu Paramount, die als erste landesweite Rechte handelt. Durch ihre schiere Größe kann das Unternehmen wesentlich kosteneffizienter arbeiten als die Mitbewerber, ganz abgesehen davon, dass dieses System auch für die Produktionsgesellschaft erhebliche Vorteile mit sich bringt. Das alte System kommt bis 1918 zum Erliegen. Kurz nach ihrer Gründung schließt Paramount Fünfjahresverträge mit Famous Players, Lasky und Bosworth ab, die später auf 25 Jahre verlängert werden. Hier zeichnet sich ein Trend ab, der 1914 zunehmend an Bedeutung gewinnt: Die Verflechtung der bisher getrennten Bereiche Distribution, Produktion und Vorführung, ein Phänomen, das in der Fachliteratur als Vertikale Integration bezeichnet wird. Die Bindung durch die Fünfjahresverträge ist vorteilhaft für alle Beteiligten: Jeder profitiert vom Erfolg des anderen. Wenn das Lasky-Programm sehr gut ist, wird das Paramount-Sortiment von mehr Kinos gekauft, wovon auch Famous Players und Bosworth profitieren, da ihr Programm so auch eine größere Verbreitung findet. Die Kooperation führt dann auch, zwei Jahre später, zur Fusion der genannten und noch einiger weiterer Unternehmen. Doch es lassen sich durchaus auch frühere Beispiele für vertikale Integration finden. So sind 1912 unter dem Namen Universal erstmals alle drei Bereiche des Filmbusiness vereint. Es fehlte allerdings eine große First-Run-Kinokette. Dennoch schien der Branche die Fusion so bedrohlich, dass die Gründung von Mutual eine direkte Gegenmaßnahme darstellen sollte. Auch hier fanden sich viele Unternehmen unter einem Dach zusammen, denen es explizit nur um Distribution und Produktion ging. Auch William Fox besitzt 1913 ein Distributions- und ein Produktionsunternehmen, die allerdings erst später zusammengeführt werden. Von Seiten der Kinokettenbesitzer ist zunächst wenig zu hören, erst 1915 schließen sich drei große Ketten, Rowland, Clarke und Mayer, zur Metro Pictures Corporation zusammen, einer Produktionsgesellschaft. Die wirklich große Reaktion der Kinobesitzer kam erst 1917. Zu diesem Zeitpunkt war die fusionierte Paramount zur dominanten Gesellschaft geworden, die ihre Filme mittels Block-Booking vertrieb. Das hieß, um einen Film mit einem Star vom Kaliber einer Mary Pickford zu bekommen, musste man ein komplettes Paket erwerben, dessen große Mehrheit bestenfalls als durchschnittlich zu bezeichnen war. Andererseits konnte man dem Kauf der Pakete schlecht entgehen, wenn man nicht sein Publikum an ein anderes Kino verlieren wollte, das ebendiesen Mary-Pickford-Film zeigte. Um dieses System zu durchbrechen, schlossen sich 26 der größten nationalen First-Run-Kinokettenbesitzer zum First National Exhibitors Circuit zusammen. Mit ihrer erheblichen Kaufkraft wollten sie gemeinsame Einkäufe tätigen und auch distribuieren. Zuerst war es das Ziel, Stars zu kaufen, ihre Filme zu finanzieren und im Gegenzug das Aufführungsrecht zu erwerben sowie das Recht, die entstandenen Filme regional weiter zu verleihen. Sehr bald kam auch eine eigene Produktion dazu. Zwischen 1917 und 1918 nahm First National Charlie Chaplin und Mary Pickford für jeweils eine Million Dollars unter Vertrag. Beide erhielten vollständige künstlerische Freiheit. First National kontrollierte zu diesem Zeitpunkt bereits ca. 600 Kinos, 200 davon Erstaufführungshäuser. Aus den First-Run-Kinos stammten bis zu 50 Prozent der Einnahmen der Produzenten, außerdem waren Kinos die verlässlichsten Geldverdiener im recht unsteten Filmgeschäft, da das Betreiberrisiko viel geringer war als beispielsweise in der Produktion. Darüber hinaus entschied der Erfolg in den First-Runs über eine lukrative Distribution. Wenn Paramount also seine Abnehmer und sein Publikum nicht verlieren wollte, musste ein Gegenschlag erfolgen. Also stieg die Gesellschaft, mit finanzieller Unterstützung des Bankhauses Kuhn, Loeb & Co., ins Geschäft mit den Kinos ein, anfangs mit einer Summe von 10 Millionen Dollar. Somit wurde Paramount der erste vollintegrierte, oder komplett vertikal integrierte Filmkonzern. So wurden aus den alten Independents die Inhaber des zweiten Oligopols. Am Ende der 1910er Jahre war der erste Punkt des freien Wettbewerbs durch das Starsystem und Feature-Filme außer Kraft gesetzt, der zweite Punkt durch die schiere Größe der Unternehmen: Weniger als zehn Unternehmen kontrollierten über 50 Prozent des Marktes. Durch die Vereinigung der Distribution und durch den beginnenden Kampf um die Kinos waren auch die letzten beiden Bedingungen für einen funktionierenden Wettbewerb ausgehebelt. Ein neues Unternehmen konnte weder einen genügenden Zugang zu den Kinos noch Zugriff auf die Stars, also auf die essentiellen Ressourcen der Filmproduktion erhalten. Auch waren die Produktionskosten stark gestiegen. Zwischen 50.000 und 100.000 US-Dollar pro Film waren normal, nach oben gab es keine Beschränkungen. Ein Großteil dieses Geldes floss in die Taschen der Stars, der Rest wurde in bessere Ausstattung investiert, eine weitere Hürde für Neueinsteiger. Um dem Trend zu höheren Gagen entgegenzuwirken, und um, wie später in einer Anhörung des Obersten Gerichtshofs bekannt wurde, ein Monopol zu errichten, planten First National und Paramount eine Fusion im Wert von 40 Millionen US-Dollar. Es war geplant, mit jedem bedeutenden Kinobesitzer in den Vereinigten Staaten einen Fünf-Jahres-Vertrag abzuschließen. Die Stars hätten dann keine Grundlage mehr für irgendwelche Forderungen gehabt. Die Pläne zu diesem Merger wurden von einem Privatdetektiv aufgedeckt, der im Auftrag von Charlie Chaplin, Mary Pickford, Douglas Fairbanks und D. W. Griffith herausfinden sollte, warum weder First National noch Paramount ihre Verträge verlängerte. Natürlich waren sie entsetzt über solche Aussichten und beschlossen, dem entgegenzuwirken, indem sie ihr eigenes Unternehmen gründeten. 1919 entstand United Artists als Gesellschaft für den Filmvertrieb. Finanziert wurde das Unternehmen durch die Morgan-Gruppe sowie durch eine Einlage von 100.000 US-Dollar für Vorzugs-Anteilscheine durch die Eigentümer. Daneben existierten auch normale Anteilscheine, bei deren Weiterverkauf United Artists ein Vorkaufsrecht hatte. Die Gesellschaft hatte keine eigenen Studios, sondern nutzte die Studios seiner Mitglieder. Sie war errichtet worden als reine Dienstleistungsgesellschaft, die nicht auf Rendite arbeiten sollte, sondern den Besitzern größtmögliche Autonomie und Profite aus dem Geschäft mit ihren Filmen einräumte. Es gab kein Block-Booking, jeder Film wurde individuell vertrieben und musste allein durch seine künstlerischen Qualitäten überzeugen. Die Verleihgebühren der United Artists lagen deutlich unter denen von First National und Paramount, stellten also eine erhebliche Bedrohung für die marktbeherrschende Stellung der beiden dar. Die Fusion der beiden Giganten war auch gescheitert, weil ihr wichtigstes Kapital, die Stars, sich auf und davon gemacht hatte. First National war also immer noch Konkurrent Paramounts, und die United Artists mit ihren qualitativ sehr hochwertigen Filmen und ihrer enormen Beliebtheit brachten das Unternehmen weiter in Bedrängnis. Also versuchte Paramount das, was man heute eine feindliche Übernahme nennen würde: Stück für Stück wurden die in der First National zusammengeschlossenen Kinoketten aufgekauft. Auch andere Unternehmen versuchten nun, Kontrolle über die Erstaufführungshäuser zu erlangen, sogar United Artists sah sich später, 1924, mangels Abnehmern gezwungen, eine eigene Kette zu gründen. Wie auch schon in der Vergangenheit, wurden die Kämpfe um die Kinos mit harten Bandagen ausgetragen, vor allem Paramounts „dynamite gang“, auch „wrecking crew“ genannt, wurde ihrem Ruf gerecht. Eine weit verbreitete Methode, Kinos an sich zu binden, war das Blocksystem. Seit 1917 begannen US-amerikanische Unternehmen, ihre Gewinne auf der Basis von in- und ausländischen Verkäufen zu schätzen. Aus dieser Gewinnschätzung ergab sich das Budget der Produktion, das dadurch erhöht wurde, was für die ausländische Konkurrenz doppelt schlecht war. Die Produktionskosten eines Filmes wurden in den Vereinigten Staaten amortisiert, und später wurden die Filme billig im Ausland angeboten, wodurch die internationale Konkurrenz nicht mehr mithalten konnte. US-amerikanische Filme galten als qualitativ besser und waren im Erwerb trotzdem günstiger als z. B. deutsche Produktionen. Auch waren die Infrastruktur und die Rationalisierung der Produktionsabläufe nirgends so weit gediehen wie in Hollywood, ein Resultat auch des wachsenden Einflusses der Banken. Als der Erste Weltkrieg vorbei war, und die Menschen in den bislang abgeschnittenen Ländern wie Deutschland oder Österreich erstmals wieder Hollywood-Produktionen zu sehen bekamen, erlebten sie einen wahren Quantensprung in der Qualität. Die führenden europäischen Filmproduktionsländer, deren isolierte Filmindustrien fünf Jahre lang unter dem Ersten Weltkrieg gelitten hatten, und zudem mit viel geringeren Budgets zu kämpfen hatten, konnten der Konkurrenz aus den Vereinigten Staaten nur noch wenig entgegensetzen. Bis 1927 erhöhte sich der Anteil der amerikanischen Filmproduktion an der Weltfilmproduktion auf nahezu 90 %, was zu Beginn der 1920er Jahre die Filmwirtschaft in England, Frankreich, Italien, Deutschland und Österreich schwer in Bedrängnis brachte und die dortige Filmproduktion stark zurückgehen ließ. Zahlreiche europäische Filmproduktionsgesellschaften mussten schließen. 1925 wurden alleine nach Österreich 1200 US-Produktionen exportiert, obwohl der Bedarf der dortigen Kinos auf lediglich rund 350 geschätzt wurde. In vielen Ländern wurden Filmkontingente eingeführt, die die erlaubte Anzahl an Filmimporten aus den Vereinigten Staaten regelten. Da rund 45 % der Gewinne zu dieser Zeit aus Europa kamen, wurden die Restriktionen in Europa von den amerikanischen Filmmagnaten mit Argwohn betrachtet. Zumeist erfolglos wurde gegen Einfuhrbeschränkungen Lobbying betrieben. In Ungarn jedoch wurden die geplanten Einfuhrbeschränkungen nicht eingeführt, nachdem die US-amerikanische Filmindustrie den ungarischen Behörden damit gedroht hatte, keine Filme mehr in Ungarn zu zeigen. 1927 waren nach Zahlen des US-Handelsdepartements beim amerikanischen Film 350.000 Personen beschäftigt. Zur Filmproduktion wurden rund 500.000 Kilometer Filmband verbraucht, wofür mehr Silber benötigt wurde, als der Umlauf an Silbermünzen in den Vereinigten Staaten ausmachte. Es wurden Filme im Ausmaß von 75.000 Kilometer Filmband und einem damaligen Wert von rund 320 Millionen Mark exportiert. Ende des Jahres 1927 zählten die Vereinigten Staaten 21.642 Kinos, die in jenem Jahr insgesamt 3 Milliarden Mal besucht wurden, was wiederum einen Erlös aus dem Eintrittsgeld von rund 2,5 Milliarden Dollar ergab. Während Amerika den weltweiten Filmmarkt fast ohne nennenswerte Konkurrenz dominierte, hatten ausländische Produktionen am US-Markt kaum eine Chance. Spielten in manchen Ländern jährlich bis zu 1000 oder mehr US-Filmproduktionen in den Kinos, liefen in den gesamten Vereinigten Staaten im Jahr 1927 nur 65 ausländische Filme, davon 38 aus Deutschland, neun aus England, sechs aus Frankreich, vier aus Russland, je zwei aus Österreich und Italien und je einer aus China und Polen. Selbst diese Filme waren zumeist nur wenig verbreitet und liefen fast ausschließlich auf so genannten Filmkunstbühnen. Ab 1933, verstärkt jedoch ab Beginn des Zweiten Weltkriegs und der Ausbreitung des Deutschen Reichs auf immer weitere Teile Europas, setzte eine Emigrationswelle von zumeist jüdischen Filmschaffenden aus Europa ein. Waren deren Auswanderungsziele zu Beginn noch häufig europäische Städte mit Filmindustrie wie Wien, Paris oder London, kristallisierte sich bald die aufstrebende Filmindustrie Hollywoods als begehrtestes und vielversprechendstes Ziel der Emigranten heraus – verstärkt durch gezieltes Anwerben europäischer Filmgrößen durch Hollywood-Studiobosse. Von den etwa 2000 jüdischen Filmschaffenden, die im Deutschen Reich keine Arbeit mehr fanden und auswandern mussten, fanden sich letztendlich rund 800 in Hollywood wieder – darunter fast die gesamte Elite des deutschsprachigen Filmschaffens dieser Zeit. Vielen gelang dort eine ruhmvolle Karriere, viele, vor allem jene, die 1938 und noch später ohne Arbeitsangebot in Hollywood ankamen, konnten nicht mehr an ihre bisherige Karriere anschließen und kamen nur in schlecht bezahlten und unbedeutenden Positionen unter oder mussten nach einer Weile gar das Filmgeschäft aufgeben. Statt der bisher aus Berlin und Wien gewohnten Kaffeehäuser, wo man sich einst regelmäßig traf, wurden nun große Appartements und Villen von in Hollywood erfolgreichen Emigranten neue Treffpunkte. Beliebte Treffpunkte der Film- und Theaterschaffenden waren die Adressen von Henry Koster, Paul Henreid, Ernst Deutsch-Dryden, Paul Kohner und später auch von Sam Spiegel. Die literarische Emigration, inklusive Drehbuchautoren, traf sich häufig bei Salka Viertel und bei Brecht.
SI-Präfixe sind für die Verwendung im Internationalen Einheitensystem (SI) definierte '''Dezimalpräfixe'''. Sie basieren auf Zehnerpotenzen mit ganzzahligen Exponenten. Man unterscheidet zwischen dem Namen des Präfixes und seinem Symbol. Die Symbole sind international einheitlich. Die Namen unterscheiden sich je nach Sprache. Die Zeichen für Teile einer Einheit werden als Kleinbuchstaben geschrieben, während die meisten Zeichen für Vielfache einer Einheit als Großbuchstaben geschrieben werden. Ausnahmen von dieser Systematik sind aus historischen Gründen die Zeichen für Deka (da), Hekto (h) und Kilo (k). Die Einheitenvorsatzzeichen werden wie die Einheitenzeichen in aufrechter, nicht kursiver Schrift geschrieben, unabhängig von der Schriftart (Schriftauszeichnung) des umgebenden Textes. Zwischen Einheitenvorsatzzeichen und Einheitenzeichen wird kein Zwischenraum geschrieben. Das Zeichen „μ“ stammt als einziges Präfix-Symbol aus der griechischen Schrift, was beim Maschinenschreiben und Drucken Schwierigkeiten bereitet hat. In der elektrotechnischen Literatur wurde deshalb ersatzweise häufig ein „u“ verwendet. Das wurde in der Internationalen Norm ISO 2955 von 1983, die 2001 zurückgezogen wurde, auch so empfohlen. Für Deutschland gelten weiterhin die Empfehlungen der DIN-Norm DIN 66030 „Informationstechnik – Darstellung von Einheitennamen in Systemen mit beschränktem Schriftzeichenvorrat“ vom Mai 2002. In Österreich sieht das Maß- und Eichgesetz „μ“ vor. Beim Austausch medizinischer Daten gemäß dem HL7-Standard ist das „u“ anstelle von „μ“ zugelassen. Der Name eines Einheitenvorsatzes bildet mit dem zugehörigen Einheitennamen ein zusammengesetztes Wort. Beispiele sind ''Nanometer'' oder ''Milligramm.'' Wenn aus dem Zusammenhang klar ist, welche Einheit gemeint ist, wird dieses zusammengesetzte Wort in der Umgangssprache häufig auf den Vorsatz verkürzt. So ist von ''Kilo'' die Rede, wenn ''Kilogramm'' (kg) gemeint ist. Im technischen Bereich wird der Mikrometer (μm) kurz als ''My''  bezeichnet; im Englischen ist die Bezeichnung ''micron'' für Mikrometer üblich. Im Österreichischen wird das Kurzwort ''Deka'' für die Masseeinheit ''Dekagramm'' (dag) verwendet, unter Handwerkern auch ''Zenti'' für Zentimeter (cm). Im Flächenmaß Hektar verschwindet ausnahmsweise an der Wortfügestelle das „o“ von „hekto“, was den Doppelselbstlaut vermeidet. Bis 1960 waren in Frankreich die Vorsätze „Myria“ (gr.   = zehntausend) mit dem Zeichen ma für das 10+4‑fache und ''dimi'' mit Zeichen dm für das 10−4‑fache genormt. Statt ''myria'' wurde Anfang des 19. Jahrhunderts auf einen Vorschlag von Thomas Young hin z. T. auch ''myrio'' geschrieben. Bis um 1900 wurde im Deutschen „Centimeter“ mit C geschrieben. Früher waren in Deutschland auch das Symbol D und in Großbritannien dk für ''Deka'' üblich, in Österreich war das Zeichen dk bis Mitte der 1950er Jahre gesetzlich vorgeschrieben. In DIN 1301 Teil 1 vom Dezember 1993 wurde der SI-Vorsatz für 10−24 „Yocto“ geschrieben; diese Schreibweise wurde in der Ausgabe vom Oktober 2002 zu „Yokto“ korrigiert. Bis 1950 wurden die elektrische Kapazität von Kondensatoren, aber auch die Selbstinduktion von Spulen in cm (Centimeter) des CGS-Einheitensystems angegeben. Bisweilen wurde pF (Piko-Farad) auch als μμF geschrieben. Der besseren Lesbarkeit wegen findet sich auf oft kleinen Bauteilen statt μF gelegentlich uF (siehe dazu Abschnitt ''Typographie''), MF oder (im Englischen) MFD.; geläufig ist auch KV statt kV für Spannung und MEGOHM statt MΩ für Widerstand. Im Sprachgebrauch von Internetbenutzern wird mitunter das SI-Präfix k in unüblichen Kontexten verwendet, z. B. bei Zeit- und Stückzahlangaben. Vergleiche auch den besonders speziellen Fall der Bezeichnung Y2K für das Jahr-2000-Problem oder W2K für Microsoft Windows 2000. Im kaufmännisch-technischen Umfeld wird das Präfix k außerdem häufig mit Währungseinheiten verwendet, etwa als k€. Die dort ebenfalls verwendete Kombination T€ stammt nicht aus dem SI, sondern bedeutet „Tausend Euro“. In der Datenverarbeitung werden SI-Präfixe auch für Datenmengen (Bits und Bytes) verwendet, allerdings oft in der Bedeutung als Binärpräfix (Vielfache von 1024, z. B. 210, 220, 230 usw.). Bis heute werden bei Datenmengen je nach Kontext, unter Umständen je nach betrachtetem Speichermedium, die SI-Präfixe als Dezimalpräfixe oder Binärpräfixe verwendet, was insbesondere bei höheren Werten zu erheblichen Abweichungen führt. Die für die Normung in der Elektrotechnik zuständige International Electrotechnical Commission hat daher zuerst in der Norm IEC 60027-2 (ersetzt durch IEC 80000-13:2008) besondere, an die SI-Präfixe angelehnte Binärpräfixe gemäß unten stehender Tabelle definiert und empfiehlt deren Verwendung für Datenmengen. Die dezimalen SI-Präfixe sollen bei Datenmengen das gleiche bedeuten wie bei SI-Einheiten (Dezimalpräfixe). Das für die SI-Präfixe zuständige Internationale Büro für Maß und Gewicht (BIPM) empfiehlt ebenfalls die Anwendung dieser Norm: Das binäre Präfixsymbol entsteht durch das Anhängen von -i an das entsprechende dezimale Präfixsymbol. Ki wird dabei im Gegensatz zu k groß geschrieben. Das binäre Präfix selbst entsteht durch das Anhängen von -bi an die ersten beiden Buchstaben des entsprechenden dezimalen Präfixes. Beispiel für die Verwendung: Ein Kibibyte wird geschrieben als 1 KiB = 210 B = 1024 B, wobei B für Byte steht.
